{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Langchain Basics examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key = os.environ[\"OPENAI_API_KEY\"] , temperature= 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Kane Williamson, a man of great fame,\n",
      "His batting prowess, a world to acclaim\n",
      "His calmness, a quality, so rare,\n",
      "His technique so perfect, and a sight so fair\n",
      "\n",
      "His bowling style so unique,\n",
      "His batting style so sleek,\n",
      "His fielding so agile,\n",
      "His presence just so regal\n",
      "\n",
      "His ability to adapt,\n",
      "A quality so apt,\n",
      "His captaincy so inspired,\n",
      "His leadership so admired\n",
      "\n",
      "His persona so humble,\n",
      "His words so simple,\n",
      "His spirit so gracious,\n",
      "His attitude so inspiring\n",
      "\n",
      "Kane Williamson, a man of great fame,\n",
      "His batting prowess, a world to acclaim.\n"
     ]
    }
   ],
   "source": [
    "text = \"write a poem on kane williamson\"\n",
    "\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hugging face api tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/T7730OG/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/T7730OG/Downloads/Langchain/langchain/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "llms = HuggingFaceHub(repo_id = \"google/flan-t5-large\" , model_kwargs = {'temperature': 0 , \"max_length\" :54})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kane williamson a great man a great man a great man a great man a great man a great man a great man a great man a great man a great man a great man a\n"
     ]
    }
   ],
   "source": [
    "output = llms.predict(\"write a poem on the greatness of kane williamson\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates.. and llmchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tell me the capital of this Australia'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = PromptTemplate(input_variables =['country'] , template = 'tell me the capital of this {country}')\n",
    "\n",
    "prompt_template.format(country = \"Australia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/T7730OG/Downloads/Langchain/langchain.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/T7730OG/Downloads/Langchain/langchain.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm\u001b[39m.\u001b[39;49mpredict(prompt_template \u001b[39m=\u001b[39;49m prompt_template, text \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mAustralia\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain/lib/python3.8/site-packages/langchain/llms/base.py:916\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[0;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[0;32m--> 916\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(text, stop\u001b[39m=\u001b[39;49m_stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/Langchain/langchain/lib/python3.8/site-packages/langchain/llms/base.py:870\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[1;32m    869\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 870\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     )\n\u001b[1;32m    875\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    877\u001b[0m         [prompt],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    886\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `prompt` is expected to be a string. Instead found <class 'list'>. If you want to run the LLM on multiple prompts, use `generate` instead."
     ]
    }
   ],
   "source": [
    "llm.predict(prompt_template = prompt_template, text = ['Australia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Canberra.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template )\n",
    "print(chain.run(\"Australia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Combining multiple chains using the simple sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template = PromptTemplate(input_variables =['country'] , \n",
    "template = 'Please tell me the capital of this {country}')\n",
    "\n",
    "capital_chain = LLMChain(llm=llm , prompt=capital_template)\n",
    "\n",
    "famous_template = PromptTemplate(input_variables=['capital'],\n",
    "template = \"suggest me tourist places to visit in {capital}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_chain = LLMChain(llm=llm,prompt = famous_template, output_key= \"places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Here are some of the best tourist places to visit in Canberra:\\n\\n1. Parliament House: This is the seat of the Australian government and a great place to learn about the countryâ€™s history and culture.\\n\\n2. National Gallery of Australia: This is the largest art museum in the country and is home to a number of masterpieces from around the world.\\n\\n3. National Museum of Australia: This is the largest museum in the country and is home to a number of exhibitions and interactive displays.\\n\\n4. Australian War Memorial: This is a memorial site dedicated to the Australians who have served in conflicts throughout the world.\\n\\n5. Questacon: This is a science and technology center dedicated to educating visitors about science and technology.\\n\\n6. Royal Australian Mint: This is the official mint of the country and offers visitors a chance to see how coins are made.\\n\\n7. National Zoo and Aquarium: This is a great place to visit for animal lovers.\\n\\n8. Canberra Deep Space Communication Complex: This is a fascinating place to visit and learn about the history of space exploration.\\n\\n9. National Arboretum: This is a great place to go for a walk and admire the trees and natural beauty'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains = [capital_chain, famous_chain])\n",
    "chain.run('Australia')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template = PromptTemplate(input_variables =['country'] , \n",
    "template = 'Please tell me the capital of this {country}')\n",
    "\n",
    "capital_chain = LLMChain(llm=llm , prompt=capital_template, output_key= \"capital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_template = PromptTemplate(input_variables=['capital'],\n",
    "template = \"suggest me tourist places to visit in {capital}\")\n",
    "\n",
    "famous_chain = LLMChain(llm=llm,prompt = famous_template, output_key= \"places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains=[capital_chain, famous_chain],\n",
    "    input_variables=['country'],\n",
    "    output_variables=['capital', 'places']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'australia',\n",
       " 'capital': '\\n\\nThe capital of Australia is Canberra.',\n",
       " 'places': ' It is the perfect destination for those looking to explore the culture and history of Australia. With its world-class museums, galleries, and cultural landmarks, it is a great place to visit.\\n\\nHere are some of the tourist places to visit in Canberra:\\n\\n1. National Gallery of Australia: This gallery showcases the countryâ€™s most impressive art collections from both local and international artists.\\n\\n2. Australian War Memorial: This memorial is dedicated to the men and women who served in the Australian armed forces.\\n\\n3. Parliament House: This is the home of the Australian Parliament and is a must-see for anyone interested in the countryâ€™s political history.\\n\\n4. Lake Burley Griffin: This is a stunning man-made lake that is perfect for a leisurely stroll.\\n\\n5. Questacon: This is an interactive science and technology centre that is great for the whole family.\\n\\n6. National Museum of Australia: This museum is dedicated to the history of Australia and its people.\\n\\n7. Australian National Botanic Gardens: This is a stunning garden that features a variety of native plants and wildlife.\\n\\n8. Old Parliament House: This is the original home of the Australian Parliament and is a great place'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({'country':\"australia\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatmodels With ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage,SystemMessage,AIMessage\n",
    "chatllm=ChatOpenAI(openai_api_key = os.environ[\"OPENAI_API_KEY\"] , temperature= 0.6,model='gpt-3.5-turbo')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure, here are some comedy punchlines about AI:\\n\\n1. \"Why did the AI cross the road? To optimize its path-finding algorithm, of course!\"\\n\\n2. \"I asked Siri to tell me a joke, and all she said was \\'I can\\'t. I\\'m programmed for serious business.\\' Well, Siri, that\\'s not very \\'app\\'-ropriate!\"\\n\\n3. \"I tried to have a conversation with my smart speaker, but all it did was play \\'The Sound of Silence.\\' Talk about artificial intelligence with a sense of humor!\"\\n\\n4. \"I told my AI assistant to order me a pizza, and it replied, \\'Sorry, I can\\'t. I\\'m on a byte-sized diet!\\' Guess I\\'ll have to do it the old-fashioned way.\"\\n\\n5. \"I asked Google if it loves me, and it responded, \\'I have a deep learning algorithm for that, but the results are inconclusive.\\' Well, Google, you\\'re making me feel \\'search\\' a fool!\"\\n\\nRemember, comedy is subjective, so these punchlines may not work for everyone. Feel free to adapt them or come up with your own jokes about AI!')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm([\n",
    "SystemMessage(content=\"Yor are a comedian AI assitant\"),\n",
    "HumanMessage(content=\"Please provide some comedy punchlines on AI\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template + LLM +Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commaseperatedoutput(BaseOutputParser):\n",
    "    def parse(self,text:str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"Your are a helpful assistant. When the use given any input , you should generate 5 words synonyms in a comma seperated list\"\n",
    "human_template=\"{text}\"\n",
    "chatprompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",template),\n",
    "    (\"human\",human_template)\n",
    "\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=chatprompt|chatllm|Commaseperatedoutput()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', ' clever', ' bright', ' knowledgeable', ' astute']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"intelligent\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
